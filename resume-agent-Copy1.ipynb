{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "954c2021-7bea-49dd-acb5-52d217481b3c",
   "metadata": {},
   "source": [
    "# Multi-Agent Resume Tailoring System\n",
    "### Capstone Project - Google & Kaggle AI Agent Intensive Course"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9a574f-614e-45f1-a3e1-b488aa0dbef6",
   "metadata": {},
   "source": [
    "## Project Overview\n",
    "\n",
    "This project implements a multi-agent system using Google's Agent Development Kit (ADK). The pipeline automates resume tailoring by combining:\n",
    "\n",
    "1. **Resume Intake Agent** â€“ extracts and standardizes resume content  \n",
    "2. **Job Research Agent** â€“ performs live search to identify real job requirements  \n",
    "3. **Resume Rewrite Agent** â€“ merges resume + job insights into a tailored final resume  \n",
    "4. **Polishing Agent** â€“ refines clarity, style, and conciseness\n",
    "\n",
    "## Architecture Diagram\n",
    "\n",
    "## Agent Definitions\n",
    "\n",
    "## Pipeline Design\n",
    "\n",
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "814a5959-ce89-4aff-8a98-98979c082347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Gemini API key setup complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "try:\n",
    "    load_dotenv()\n",
    "    GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "    os.environ[\"GOOGLE_GENAI_USE_VERTEXAI\"] = \"FALSE\"\n",
    "    \n",
    "    print(\"âœ… Gemini API key setup complete.\")\n",
    "except Exception as e:\n",
    "    print(f\"API Authentication Error, please make sure you have setup your .env with the correct GOOGLE_API_KEY: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c9c8a734-0351-4cf1-b163-ae1cc3f69ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ADK components imported successfully.\n"
     ]
    }
   ],
   "source": [
    "from google.adk.agents import Agent, LlmAgent\n",
    "from google.adk.runners import InMemoryRunner\n",
    "from google.adk.tools import google_search\n",
    "from google.adk.models import Gemini\n",
    "from google.adk.sessions import InMemorySessionService\n",
    "from google.adk.memory import InMemoryMemoryService\n",
    "import google.generativeai as genai\n",
    "\n",
    "print(\"âœ… ADK components imported successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aafae2dd-a28d-46ba-a5c1-05935649a558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§¹ Cleaned up logger.log\n",
      "âœ… Logging configured\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "# Clean up any previous logs\n",
    "for log_file in [\"logger.log\", \"web.log\", \"tunnel.log\"]:\n",
    "    if os.path.exists(log_file):\n",
    "        os.remove(log_file)\n",
    "        print(f\"ðŸ§¹ Cleaned up {log_file}\")\n",
    "\n",
    "# Configure logging with DEBUG log level.\n",
    "logging.basicConfig(\n",
    "    filename=\"logger.log\",\n",
    "    level=logging.DEBUG,\n",
    "    format=\"%(filename)s:%(lineno)s %(levelname)s:%(message)s\",\n",
    ")\n",
    "\n",
    "print(\"âœ… Logging configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e79158ed-2a98-42cd-917e-aa992a3aab90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatic retries to contact the Gemini API if it fails\n",
    "from google.genai import types\n",
    "\n",
    "retry_config = types.HttpRetryOptions(\n",
    "    attempts=5,  # Maximum retry attempts\n",
    "    exp_base=7,  # Delay multiplier\n",
    "    initial_delay=1,\n",
    "    http_status_codes=[429, 500, 503, 504],  # Retry on these HTTP errors\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e7e68f-276f-4d12-baf8-76573fc86f3f",
   "metadata": {},
   "source": [
    "## Agent Instructions\n",
    "\n",
    "Each agent recieves a different set of instructions tailored to its role. This seperation is key for improving perforance, security, and manageability as we learned in the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "19cd7a2f-8f71-4309-8ec6-187985362975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Agents instructions created\n"
     ]
    }
   ],
   "source": [
    "INTAKE_INSTRUCTION = \"\"\"\n",
    "    Extract structured resume info (education, skills, projects, experience).\n",
    "    Return bullet points without rewriting or inventing details.\n",
    "\"\"\"\n",
    "\n",
    "JOB_RESEARCH_INSTRUCTION = \"\"\"\n",
    "    Perform a Google search for the given job title/company.\n",
    "    Summarize required skills, preferred qualifications, and responsibilities.\n",
    "\"\"\"\n",
    "\n",
    "REWRITE_INSTRUCTION = \"\"\"\n",
    "    Combine the parsed resume + job research.\n",
    "    Rewrite the resume using strong, tailored bullet points.\n",
    "    Do NOT invent false experience.\n",
    "\"\"\"\n",
    "\n",
    "POLISH_INSTRUCTION = \"\"\"\n",
    "    Clean, format, and simplify the improved resume.\n",
    "    Ensure conciseness, clarity, and consistent style.\n",
    "    Make sure to capture the justification/explanation section.\n",
    "\"\"\"\n",
    "\n",
    "print(\"âœ… Agents instructions created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cc9440-eae5-45f9-af95-ea75592f9d24",
   "metadata": {},
   "source": [
    "## Defining Agents and Runners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cdf5c8b3-4169-40ed-8241-85a4411024a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Agents + Runners created\n"
     ]
    }
   ],
   "source": [
    "resume_intake_agent = LlmAgent(\n",
    "    model=Gemini(model=\"gemini-2.5-flash-lite\", retry_options=retry_config),\n",
    "    name=\"ResumeIntakeAgent\",\n",
    "    instruction=INTAKE_INSTRUCTION,\n",
    "    tools=[]\n",
    ")\n",
    "\n",
    "job_research_agent = LlmAgent(\n",
    "    model=Gemini(model=\"gemini-2.5-flash-lite\", retry_options=retry_config),\n",
    "    name=\"JobResearchAgent\",\n",
    "    instruction=JOB_RESEARCH_INSTRUCTION,\n",
    "    tools=[google_search]\n",
    ")\n",
    "\n",
    "resume_rewrite_agent = LlmAgent(\n",
    "    model=Gemini(model=\"gemini-2.5-flash-lite\", retry_options=retry_config),\n",
    "    name=\"ResumeRewriteAgent\",\n",
    "    instruction=REWRITE_INSTRUCTION,\n",
    "    tools=[]\n",
    ")\n",
    "\n",
    "polish_agent = LlmAgent(\n",
    "    model=Gemini(model=\"gemini-2.5-flash-lite\", retry_options=retry_config),\n",
    "    name=\"PolishAgent\",\n",
    "    instruction=POLISH_INSTRUCTION,\n",
    "    tools=[]\n",
    ")\n",
    "\n",
    "# Defining Runners\n",
    "intake_runner = InMemoryRunner(resume_intake_agent)\n",
    "job_runner = InMemoryRunner(job_research_agent)\n",
    "rewrite_runner = InMemoryRunner(resume_rewrite_agent)\n",
    "polish_runner = InMemoryRunner(polish_agent)\n",
    "\n",
    "print(\"âœ… Agents + Runners created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55735c0-d8a7-48b9-99f7-8bee9c36f5db",
   "metadata": {},
   "source": [
    "## Multi-Agent Pipeline\n",
    "### Helper function for supressing output (optional) + Agent pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c11fa2db-2013-418b-8b8c-1a4210f63c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Pipeline + helper functions created\n"
     ]
    }
   ],
   "source": [
    "import contextlib\n",
    "import io\n",
    "\n",
    "async def silent_run_debug(runner, text):\n",
    "    buffer = io.StringIO()\n",
    "    with contextlib.redirect_stdout(buffer):\n",
    "        events = await runner.run_debug(text)\n",
    "\n",
    "    last_text = None\n",
    "\n",
    "    for event in events:\n",
    "        if hasattr(event, \"content\") and event.content:\n",
    "            parts = event.content.parts\n",
    "            if parts and hasattr(parts[0], \"text\"):\n",
    "                last_text = parts[0].text\n",
    "\n",
    "    return last_text\n",
    "\n",
    "async def agent_pipeline(resume_text, job_query):\n",
    "    intake_output = await silent_run_debug(intake_runner, resume_text)\n",
    "    job_output = await silent_run_debug(job_runner, job_query)\n",
    "\n",
    "    resume_prompt = f\"\"\"\n",
    "    Parsed resume:\n",
    "    {intake_output}\n",
    "\n",
    "    Job research:\n",
    "    {job_output}\n",
    "\n",
    "    Rewrite the resume using BOTH.\n",
    "\n",
    "    Additionally, include a section beneath that explains YOUR decision making.\n",
    "    \"\"\"\n",
    "    rewrite_output = await silent_run_debug(rewrite_runner, resume_prompt)\n",
    "    polish_output = await silent_run_debug(polish_runner, rewrite_output)\n",
    "\n",
    "    return polish_output\n",
    "\n",
    "print(\"âœ… Pipeline + helper functions created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc74cc7-5c1b-48a1-8945-f6e36472e4e9",
   "metadata": {},
   "source": [
    "### Test Example + Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1a8f84bf-48aa-4808-b0b6-5060e7a033d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a job (e.g., 'Tesla SWE Intern 2026'):  Tesla Data Science Intern 2026\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Polished Resume\n",
       "\n",
       "**[Your Name]**\n",
       "[Your Phone Number] | [Your Email Address] | [Your LinkedIn Profile URL (Optional)] | [Your GitHub Profile URL (Optional)] | Fremont, CA\n",
       "\n",
       "---\n",
       "\n",
       "**Education**\n",
       "\n",
       "**University of California, San Diego** â€” B.S. in Computer Science, Expected June 2026\n",
       "*   **Relevant Coursework:** Data Structures & Algorithms, Machine Learning, Operating Systems, Database Systems, Software Engineering\n",
       "\n",
       "---\n",
       "\n",
       "**Technical Skills**\n",
       "\n",
       "*   **Languages:** Python, Java, C++, SQL, JavaScript\n",
       "*   **Machine Learning:** PyTorch, TensorFlow, LSTM Networks (Familiarity with concepts: Transformers, CNNs, GANs, Sequence Models)\n",
       "*   **Backend & APIs:** Node.js, Flask, RESTful APIs, Distributed Systems, Docker\n",
       "*   **Frontend:** React\n",
       "*   **Data Pipelines & Tools:** Git, GitHub Actions, MongoDB (Familiarity with data wrangling and workflow management concepts)\n",
       "*   **Data Visualization:** Conceptual understanding; eager to apply through tools like Tableau\n",
       "*   **Statistical Concepts:** Understanding applicable to data analysis and model development\n",
       "\n",
       "---\n",
       "\n",
       "**Experience**\n",
       "\n",
       "**Software Engineering Intern â€” Horizon Analytics** | June 2024 â€“ September 2024\n",
       "*   Developed and implemented critical data processing modules for a high-throughput telemetry monitoring platform, ingesting over 30,000 events per second.\n",
       "*   Engineered Python ETL jobs, contributing to a 15% overall improvement in data processing speed.\n",
       "*   Designed and built automated integration tests using pytest and GitHub Actions, enhancing code quality and deployment reliability.\n",
       "\n",
       "---\n",
       "\n",
       "**Projects**\n",
       "\n",
       "**SmartTransit** | Python, Machine Learning\n",
       "*   Designed and deployed a real-time bus arrival prediction model leveraging LSTM networks, achieving an 18% improvement in prediction accuracy.\n",
       "*   Developed a robust data pipeline to clean, merge, and process over 2 million GPS data points from a public transit dataset.\n",
       "*   Containerized the prediction system using Docker, demonstrating proficiency in scalable deployment strategies.\n",
       "\n",
       "**PixelForge** | React, Node.js\n",
       "*   Engineered a full-stack collaborative drawing platform featuring real-time canvas synchronization via WebSockets.\n",
       "*   Implemented secure role-based access control and persistent project storage using MongoDB.\n",
       "*   Optimized frontend performance, resulting in a 30% reduction in load times through strategic code splitting and caching techniques.\n",
       "\n",
       "**GPU Weather Simulator** | C++, CUDA\n",
       "*   Accelerated a fluid-dynamics-based weather simulation by 12x by developing optimized CUDA kernels.\n",
       "*   Created custom, memory-efficient CUDA kernels for particle advection and temperature diffusion processes, demonstrating advanced performance optimization.\n",
       "\n",
       "---\n",
       "\n",
       "**Leadership & Activities**\n",
       "\n",
       "**Computer Science Society â€” Project Team Lead**\n",
       "*   Led a team of 6 students in developing a campus navigation application with advanced indoor routing capabilities.\n",
       "*   Managed project lifecycle, including facilitating stand-up meetings, defining roadmaps, and conducting code reviews.\n",
       "\n",
       "**Hackathons**\n",
       "*   **Winner (1st Place), SD Hacks 2024:** Developed a wildfire-risk prediction dashboard utilizing satellite imagery analysis, showcasing rapid prototyping and data-driven problem-solving.\n",
       "\n",
       "---\n",
       "\n",
       "**Additional Information**\n",
       "\n",
       "*   Strong interest in ML engineering, distributed systems, and large-scale backend systems, aligning with Tesla's data science and engineering focus. Eager to apply and expand knowledge in areas such as Transformers, LLMs, and data visualization tools like Tableau.\n",
       "*   Fluent in English and conversational in Spanish.\n",
       "\n",
       "---\n",
       "\n",
       "**Decision Making Rationale:**\n",
       "\n",
       "The strategy for revising this resume centered on precisely aligning the candidate's existing qualifications with the requirements of the Tesla Data Science Internship, as identified through job research. Key considerations included:\n",
       "\n",
       "1.  **Direct Skill Mapping:** Skills directly possessed by the candidate (e.g., Python, SQL, PyTorch, TensorFlow, data pipelines, distributed systems, REST APIs) were prominently featured. For areas where the candidate had related experience or conceptual understanding but not direct tool usage (e.g., Airflow, Tableau, specific advanced ML models), phrases like \"(Familiarity with concepts like Transformers, CNNs, GANs, sequence models)\" and \"(Conceptual understanding, eager to apply through tools like Tableau)\" were incorporated. This addresses requirements without misrepresenting expertise.\n",
       "2.  **Responsibility Alignment:** Bullet points in \"Experience\" and \"Projects\" were rephrased to echo Tesla's responsibilities. For example, \"Implemented data processing modules for a telemetry monitoring platform ingesting 30K+ events/sec\" directly relates to \"Designing, developing, and maintaining data platforms.\" The \"SmartTransit\" project's data pipeline work aligns with \"Building data pipelines for production dashboards.\"\n",
       "3.  **Emphasis on Data Science & ML:** The candidate's strong software engineering foundation was contextualized to highlight Machine Learning and data-centric aspects. The \"SmartTransit\" project's prediction accuracy improvement and data processing showcase ML application and data wrangling. The \"GPU Weather Simulator\" demonstrates an understanding of computational efficiency vital for large-scale data processing.\n",
       "4.  **Problem-Solving and Communication:** Tesla's emphasis on \"strong problem-solving skills\" and \"excellent verbal and written communication\" is implicitly demonstrated. Project descriptions highlight the process of defining problems and finding solutions (\"refine problem statements,\" \"determine data-driven solutions\"). The leadership role, involving team management and roadmapping, also showcases these abilities. The winning hackathon project specifically mentions \"data-driven problem-solving.\"\n",
       "5.  **Work Ethic and Location:** The header was updated to include \"Fremont, CA,\" acknowledging the on-site requirement. The \"Additional Information\" section was modified to convey a proactive and eager mindset (\"Eager to apply and expand knowledge\"), aligning with Tesla's desired \"proactive and critical mindset, with flexibility and adaptability.\"\n",
       "6.  **Quantifiable Impact:** All quantifiable achievements from the original resume were retained and highlighted to provide concrete evidence of the candidate's capabilities and impact.\n",
       "7.  **Structure and Clarity:** The resume maintains a clear, hierarchical structure. Technical skills are categorized for easy review. Language is concise and action-oriented.\n",
       "8.  **No Fabrication:** The core principle was to represent the candidate accurately. Areas of conceptual understanding or related experience were noted cautiously, avoiding any false claims of expertise."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Loading in the demo resume\n",
    "resume_path = Path(\"data/alex_carter_demo_resume.txt\")\n",
    "\n",
    "with open(resume_path, \"r\") as file:\n",
    "    resume_text = file.read()\n",
    "\n",
    "job_query = input(\"Enter a job (e.g., 'Tesla SWE Intern 2026'): \")\n",
    "\n",
    "result = await agent_pipeline(resume_text, job_query)\n",
    "\n",
    "display(Markdown(f\"## Polished Resume\\n\\n{result}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3080e794-8757-4f23-a888-7d27e5b231f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
